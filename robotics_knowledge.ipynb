{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda vs Omega Knowledge in Robotics\n",
    "This notebook seeks to use topic modeling over a corpus of abstracts robotics paper abstracs to explore a potential relationship between theoretical and practical knowledge in the robotics field and scientific communities generally. Current implementation uses arxiv documents but I hope to use IROS and ICRA abstracts for a more representative picture of the field.\n",
    "\n",
    "I took lots of code from https://www.kaggle.com/aiswaryaramachandran/exploring-the-growth-in-ai-using-arxiv/data?select=arxiv-metadata-oai-snapshot.json\n",
    "and from https://medium.com/@kurtsenol21/topic-modeling-lda-mallet-implementation-in-python-part-1-c493a5297ad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# import networkx\n",
    "# from networkx.algorithms.components.connected import connected_components\n",
    "\n",
    "import json\n",
    "import dask.bag as db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Robotics Papers from Arxiv Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_papers():\n",
    "    ai_category_list=['cs.RO']\n",
    "    records=db.read_text(\"/home/zach/Downloads/*.json\").map(lambda x:json.loads(x))\n",
    "    ai_docs = (records.filter(lambda x:any(ele in x['categories'] for ele in ai_category_list)==True))\n",
    "    get_metadata = lambda x: {'id': x['id'],\n",
    "                      'title': x['title'],\n",
    "                      'category':x['categories'],\n",
    "                      'abstract':x['abstract'],\n",
    "                     'version':x['versions'][-1]['created'],\n",
    "                             'doi':x[\"doi\"],\n",
    "                             'authors_parsed':x['authors_parsed']}\n",
    "\n",
    "    data=ai_docs.map(get_metadata).to_dataframe().compute()\n",
    "    data\n",
    "    data.to_excel(\"AI_ML_ArXiv_Papers.xlsx\",index=False,encoding=\"utf-8\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting IEEE Papers\n",
    "\n",
    "In this section we'll get the metadata of all IROS and ICRA papers using the IEEE Xplore API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ieee(starting_number, max_number, conference):\n",
    "    query = XPLORE('jwr3z2rwq2bdpbf43tzqep7k')\n",
    "    query.publicationTitle(conference)\n",
    "    query.startingResult(starting_number)\n",
    "    query.maximumResults(max_number)\n",
    "    query.dataType('json')\n",
    "    query.dataFormat('object')\n",
    "    iros_query = query.callAPI()\n",
    "    return iros_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IEEE_papers(conference):\n",
    "    query = XPLORE('jwr3z2rwq2bdpbf43tzqep7k')\n",
    "    query.publicationTitle(conference)\n",
    "    query.startingResult(1)\n",
    "    query.maximumResults(2)\n",
    "    query.dataType('json')\n",
    "    query.dataFormat('object')\n",
    "    iros_query = query.callAPI()\n",
    "    num_docs = iros_query['total_records']\n",
    "    \n",
    "    max_number = 200\n",
    "    docs = []\n",
    "    for i in range(int(np.floor(num_docs/200))): #int(np.floor(num_docs/200)+1)\n",
    "        starting_number = i*200+1\n",
    "        iros_query = query_ieee(starting_number, max_number, conference)\n",
    "        docs.extend(iros_query['articles'])\n",
    "    iros_query = query_ieee((i+1)*200+1, 161, conference)\n",
    "    docs.extend(iros_query['articles'])\n",
    "    df = pd.DataFrame(docs)\n",
    "    if conference == 'International Conference On Intelligent Robots and Systems':\n",
    "        name = \"IROS_papers.xlsx\"\n",
    "    else:\n",
    "        name = \"ICRA_papers.xlsx\"\n",
    "    df.to_excel(name,index=False,encoding=\"utf-8\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get IROS Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xploreapi import XPLORE\n",
    "\n",
    "# first get IROS papers\n",
    "iros_df = get_IEEE_papers('International Conference On Intelligent Robots and Systems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ICRA Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icra_df = get_IEEE_papers('International Conference on Robotics and Automation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xploreapi import XPLORE\n",
    "# query = XPLORE('jwr3z2rwq2bdpbf43tzqep7k')\n",
    "# query.publicationTitle('International Conference On Intelligent Robots and Systems')\n",
    "# query.startingResult(1)\n",
    "# query.maximumResults(2)\n",
    "# query.dataType('json')\n",
    "# query.dataFormat('object')\n",
    "# iros_query = query.callAPI()\n",
    "# num_docs = iros_query['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conference = 'International Conference On Intelligent Robots and Systems'\n",
    "# max_number = 200\n",
    "# docs = []\n",
    "# for i in range(int(np.floor(num_docs/200))): #int(np.floor(num_docs/200)+1)\n",
    "#     starting_number = i*200+1\n",
    "#     iros_query = query_ieee(starting_number, max_number, conference)\n",
    "#     docs.extend(iros_query['articles'])\n",
    "# iros_query = query_ieee((i+1)*200+1, 161, conference)\n",
    "# docs.extend(iros_query['articles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfnew = pd.DataFrame(docs)\n",
    "# dfnew.to_excel(\"IROS_papers.xlsx\",index=False,encoding=\"utf-8\")\n",
    "# dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18561"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>isbn</th>\n",
       "      <th>issn</th>\n",
       "      <th>rank</th>\n",
       "      <th>volume</th>\n",
       "      <th>authors</th>\n",
       "      <th>access_type</th>\n",
       "      <th>content_type</th>\n",
       "      <th>...</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>start_page</th>\n",
       "      <th>end_page</th>\n",
       "      <th>citing_paper_count</th>\n",
       "      <th>citing_patent_count</th>\n",
       "      <th>index_terms</th>\n",
       "      <th>isbn_formats</th>\n",
       "      <th>abstract</th>\n",
       "      <th>partnum</th>\n",
       "      <th>html_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1109/IROS.1992.594221</td>\n",
       "      <td>\"A Strategy Of Self-organization For Cellular ...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>0-7803-0737-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'authors': [{'affiliation': 'Toyo Engineering...</td>\n",
       "      <td>LOCKED</td>\n",
       "      <td>Conferences</td>\n",
       "      <td>...</td>\n",
       "      <td>7-10 July 1992</td>\n",
       "      <td>1558</td>\n",
       "      <td>1565</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>{'ieee_terms': {'terms': ['Robots', 'Hardware'...</td>\n",
       "      <td>{'isbns': [{'format': 'Print ISBN', 'value': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1109/IROS.1992.601935</td>\n",
       "      <td>\"Arnie P.\" - A Robot Golfing System Using Bino...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>0-7803-0737-2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'authors': [{'affiliation': 'Department of Co...</td>\n",
       "      <td>LOCKED</td>\n",
       "      <td>Conferences</td>\n",
       "      <td>...</td>\n",
       "      <td>7-10 July 1992</td>\n",
       "      <td>2027</td>\n",
       "      <td>2034</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'ieee_terms': {'terms': ['Feedback', 'Robot k...</td>\n",
       "      <td>{'isbns': [{'format': 'Print ISBN', 'value': '...</td>\n",
       "      <td>This paper describes a robot vision golfing sy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1109/IROS.2000.893149</td>\n",
       "      <td>\"Toy problem\" as the benchmark test for teleop...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>0-7803-6348-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>{'authors': [{'affiliation': 'Dept. of Mech. E...</td>\n",
       "      <td>LOCKED</td>\n",
       "      <td>Conferences</td>\n",
       "      <td>...</td>\n",
       "      <td>31 Oct.-5 Nov. 2000</td>\n",
       "      <td>996</td>\n",
       "      <td>1001 vol.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>{'ieee_terms': {'terms': ['Benchmark testing',...</td>\n",
       "      <td>{'isbns': [{'format': 'Print ISBN', 'value': '...</td>\n",
       "      <td>A unified hand/arm master-slave system was dev...</td>\n",
       "      <td>00CH37113</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1109/IROS.2006.282290</td>\n",
       "      <td>\"Tri-Star3\"; Horizontal Polyarticular Arm Equi...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>1-4244-0259-X</td>\n",
       "      <td>2153-0866</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'authors': [{'full_name': 'Kenjiro Tadakuma',...</td>\n",
       "      <td>LOCKED</td>\n",
       "      <td>Conferences</td>\n",
       "      <td>...</td>\n",
       "      <td>9-15 Oct. 2006</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'ieee_terms': {'terms': ['Orbital robotics', ...</td>\n",
       "      <td>{'isbns': [{'format': 'CD', 'value': '1-4244-0...</td>\n",
       "      <td>The expandable-type rover, which retracts in t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1109/IROS.2005.1544987</td>\n",
       "      <td>\"XPFCP\": an extended particle filter for track...</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>0-7803-8912-3</td>\n",
       "      <td>2153-0866</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'authors': [{'affiliation': 'Dept. of Electro...</td>\n",
       "      <td>LOCKED</td>\n",
       "      <td>Conferences</td>\n",
       "      <td>...</td>\n",
       "      <td>2-6 Aug. 2005</td>\n",
       "      <td>2474</td>\n",
       "      <td>2479</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'ieee_terms': {'terms': ['Particle filters', ...</td>\n",
       "      <td>{'isbns': [{'format': 'Print ISBN', 'value': '...</td>\n",
       "      <td>The work described in this paper explores a ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/1544987/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         doi  \\\n",
       "0   10.1109/IROS.1992.594221   \n",
       "1   10.1109/IROS.1992.601935   \n",
       "2   10.1109/IROS.2000.893149   \n",
       "3   10.1109/IROS.2006.282290   \n",
       "4  10.1109/IROS.2005.1544987   \n",
       "\n",
       "                                               title publisher           isbn  \\\n",
       "0  \"A Strategy Of Self-organization For Cellular ...      IEEE  0-7803-0737-2   \n",
       "1  \"Arnie P.\" - A Robot Golfing System Using Bino...      IEEE  0-7803-0737-2   \n",
       "2  \"Toy problem\" as the benchmark test for teleop...      IEEE  0-7803-6348-5   \n",
       "3  \"Tri-Star3\"; Horizontal Polyarticular Arm Equi...      IEEE  1-4244-0259-X   \n",
       "4  \"XPFCP\": an extended particle filter for track...      IEEE  0-7803-8912-3   \n",
       "\n",
       "        issn  rank volume                                            authors  \\\n",
       "0          1     1      3  {'authors': [{'affiliation': 'Toyo Engineering...   \n",
       "1          1     2      3  {'authors': [{'affiliation': 'Department of Co...   \n",
       "2        NaN     3      2  {'authors': [{'affiliation': 'Dept. of Mech. E...   \n",
       "3  2153-0866     4    NaN  {'authors': [{'full_name': 'Kenjiro Tadakuma',...   \n",
       "4  2153-0866     5    NaN  {'authors': [{'affiliation': 'Dept. of Electro...   \n",
       "\n",
       "  access_type content_type  ...     publication_date start_page    end_page  \\\n",
       "0      LOCKED  Conferences  ...       7-10 July 1992       1558        1565   \n",
       "1      LOCKED  Conferences  ...       7-10 July 1992       2027        2034   \n",
       "2      LOCKED  Conferences  ...  31 Oct.-5 Nov. 2000        996  1001 vol.2   \n",
       "3      LOCKED  Conferences  ...       9-15 Oct. 2006          5           5   \n",
       "4      LOCKED  Conferences  ...        2-6 Aug. 2005       2474        2479   \n",
       "\n",
       "  citing_paper_count citing_patent_count  \\\n",
       "0                  4                   0   \n",
       "1                  3                   0   \n",
       "2                  6                   0   \n",
       "3                  0                   0   \n",
       "4                 10                   0   \n",
       "\n",
       "                                         index_terms  \\\n",
       "0  {'ieee_terms': {'terms': ['Robots', 'Hardware'...   \n",
       "1  {'ieee_terms': {'terms': ['Feedback', 'Robot k...   \n",
       "2  {'ieee_terms': {'terms': ['Benchmark testing',...   \n",
       "3  {'ieee_terms': {'terms': ['Orbital robotics', ...   \n",
       "4  {'ieee_terms': {'terms': ['Particle filters', ...   \n",
       "\n",
       "                                        isbn_formats  \\\n",
       "0  {'isbns': [{'format': 'Print ISBN', 'value': '...   \n",
       "1  {'isbns': [{'format': 'Print ISBN', 'value': '...   \n",
       "2  {'isbns': [{'format': 'Print ISBN', 'value': '...   \n",
       "3  {'isbns': [{'format': 'CD', 'value': '1-4244-0...   \n",
       "4  {'isbns': [{'format': 'Print ISBN', 'value': '...   \n",
       "\n",
       "                                            abstract    partnum  \\\n",
       "0                                                NaN        NaN   \n",
       "1  This paper describes a robot vision golfing sy...        NaN   \n",
       "2  A unified hand/arm master-slave system was dev...  00CH37113   \n",
       "3  The expandable-type rover, which retracts in t...        NaN   \n",
       "4  The work described in this paper explores a ne...        NaN   \n",
       "\n",
       "                                        html_url  \n",
       "0                                            NaN  \n",
       "1                                            NaN  \n",
       "2                                            NaN  \n",
       "3                                            NaN  \n",
       "4  https://ieeexplore.ieee.org/document/1544987/  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(docs)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the id, the title,the category the paper belongs to, the date when the version was created and list of authors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Some preprocessing steps that we need to perform are:\n",
    "\n",
    "1. Extract the Date Time information from version column\n",
    "\n",
    "\n",
    "2. The authors parsed information, first and last names need to be concatenated to get one name.\n",
    "\n",
    "3. Handling Missing DOI's\n",
    "\n",
    "4. We need to look for any possible duplication in the title names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Date Time Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DateTime']=pd.to_datetime(data['version'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "data['Year'] = data['DateTime'].dt.year\n",
    "data['Date'] = data['DateTime'].dt.date\n",
    "#data=utils.extractDateFeatures(data,\"DateTime\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the ***authors_parsed*** column\n",
    "\n",
    "\n",
    "1. Concatenating the authors first and last names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_authors']=data['authors_parsed'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['authors']=data['authors_parsed'].apply(lambda authors:[(\" \".join(author)).strip() for author in authors])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing DOI \n",
    "\n",
    "In the Data, we can see that there are papers with no doi - Since Arxiv is a pre-print server, once the paper is published DOI is received. This DOI needs to be updated to Arxiv. In cases where there are no DOI - probably they were not published in any other journal or the author forgot to update the doi - hence there is no DOI available\n",
    "\n",
    "(Reference : https://academia.stackexchange.com/questions/62480/why-does-arxiv-org-not-assign-dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Papers with No DOI \",data[pd.isnull(data['doi'])].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aroung 88% of the papers have no DOI - the authors most probably didnt update this information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the Data\n",
    "\n",
    "1. How has the field of ML/AI grown over the years?\n",
    "\n",
    "2. Who have been the most successful Authors?\n",
    "\n",
    "3. What are the different topics being spoken about  - and how this has changed over the years?\n",
    "\n",
    "4. Can we cluster papers based on their Abstract and Title? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growth in Field of ML AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "papers_over_years=data.groupby(['Year']).size().reset_index().rename(columns={0:'Number Of Papers Published'})\n",
    "px.line(x=\"Year\",y=\"Number Of Papers Published\",data_frame=papers_over_years,title=\"Growth of Robotics over the Years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2010, there has been an exponential growth in this field - and this is continuously increasing over the period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_published_over_days=data.groupby(['Date']).size().reset_index().rename(columns={0:'Papers Published By Date'})\n",
    "px.line(x=\"Date\",y=\"Papers Published By Date\",data_frame=papers_published_over_days,title=\"Average Papers Published Over Each Day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From one published paper over each day, in the last one year there have been around 100 papers published each day. In Mar2013, there is a jump in number of papers published. Also, 2013 was the year, when the paper on Word2Vec was published - this was a new beginning in the field of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who has published most papers in AI ML Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_authors=pd.DataFrame(data['authors'].tolist()).rename(columns={0:'authors'})\n",
    "papers_by_authors=ai_authors.groupby(['authors']).size().reset_index().rename(columns={0:'Number of Papers Published'}).sort_values(\"Number of Papers Published\",ascending=False).head(20)\n",
    "px.bar(x=\"Number of Papers Published\",y=\"authors\",data_frame=papers_by_authors.sort_values(\"Number of Papers Published\",ascending=True),title=\"Top 20 Popular Authors\",orientation=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data['authors'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk library and then download stopwords\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data = list(data.abstract)\n",
    "abstract_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigrams\n",
    "bigram = gensim.models.Phrases(abstract_data, min_count=20, threshold=100) \n",
    "trigram = gensim.models.Phrases(bigram[abstract_data], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need tagger, no need for parser and named entity recognizer, for faster implementation\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_tags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    \"\"\"Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\"\"\"\n",
    "    \n",
    "    # remove stopwords, short tokens and letter accents \n",
    "    texts = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts]\n",
    "    \n",
    "    # bi-gram and tri-gram implementation\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "\n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_tags])\n",
    "    \n",
    "    # remove stopwords and short tokens again after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts_out]    \n",
    "    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = process_words(abstract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corpus = {}\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "  for idx, freq in corpus[i]:\n",
    "    if id2word[idx] in dict_corpus:\n",
    "      dict_corpus[id2word[idx]] += freq\n",
    "    else:\n",
    "       dict_corpus[id2word[idx]] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = pd.DataFrame.from_dict(dict_corpus, orient='index', columns=['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(dict_df['freq'], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.sort_values('freq', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = dict_df[dict_df.freq>1500].index.tolist()\n",
    "extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = process_words(abstract_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "\n",
    "id2word.filter_extremes(no_below=10, no_above=0.5)\n",
    "\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '/home/zach/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# display topics\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('Coherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_results = ldamallet[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] for topics in tm_results]\n",
    "corpus_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3)) for term, wt in ldamallet.show_topic(n, topn=20)] for n in range(0, ldamallet.num_topics)]\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame([[term for term, wt in topic] for topic in topics], columns = ['Term'+str(i) for i in range(1, 21)],\n",
    "                         index=['Topic '+str(t) for t in range(1, ldamallet.num_topics+1)]).T\n",
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic]) for topic in topics], columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, ldamallet.num_topics+1)] )\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim as gensimvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "def convertldaMalletToldaGen(mallet_model):\n",
    "    model_gensim = LdaModel(\n",
    "        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,\n",
    "        alpha=mallet_model.alpha) # original function has 'eta=0' argument\n",
    "    model_gensim.state.sstats[...] = mallet_model.wordtopics\n",
    "    model_gensim.sync_state()\n",
    "    return model_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldagensim = convertldaMalletToldaGen(ldamallet)\n",
    "vis_data = gensimvis.prepare(ldagensim, corpus, id2word, sort_topics=False)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe \n",
    "corpus_topic_df = pd.DataFrame()\n",
    "\n",
    "# get the Titles from the original dataframe\n",
    "corpus_topic_df['Title'] = data.title\n",
    "\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Terms'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "\n",
    "corpus_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic_df = corpus_topic_df.groupby('Dominant Topic').agg(\n",
    "                                  Doc_Count = ('Dominant Topic', np.size),\n",
    "                                  Total_Docs_Perc = ('Dominant Topic', np.size)).reset_index()\n",
    "\n",
    "dominant_topic_df['Total_Docs_Perc'] = dominant_topic_df['Total_Docs_Perc'].apply(lambda row: round((row*100) / len(corpus), 2))\n",
    "\n",
    "dominant_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = dominant_topic_df[[\"Dominant Topic\", \t\"Doc_Count\", \"Total_Docs_Perc\"]]\n",
    "topic_counts.columns  = [\"Dominant Topic\", \t\"Document Count\", \"Total Document Percentage\"]\n",
    "topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = pd.DataFrame.from_records([{v: k for v, k in row} for row in tm_results])\n",
    "df_weights.columns = ['Topic ' + str(i) for i in range(1,11)]\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights['Year'] = data.Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights.groupby('Year').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights['Dominant'] = df_weights.drop('Year', axis=1).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights.groupby('Year')['Dominant'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominance = df_weights.groupby('Year')['Dominant'].value_counts(normalize=True).unstack()\n",
    "df_dominance.reset_index(inplace=True)\n",
    "df_dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = df_dominance.melt(id_vars=['Year'], value_vars=['Topic ' + str(i) for i in range(1,11)], var_name='Topic', value_name='Prevelance')\n",
    "df_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='Year', y=\"Prevelance\", hue='Topic',\n",
    "data=df_melted,\n",
    "kind=\"line\",\n",
    "height=10,\n",
    "style=\"Topic\",\n",
    "dashes=False,\n",
    "ci=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a progress meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary, start_topic_count=2, end_topic_count=10, step=1, cpus=1):\n",
    "  models = []\n",
    "  coherence_scores = []\n",
    "  for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "    mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=mallet_path, corpus=corpus, num_topics=topic_nums,\n",
    "                                                            id2word=dictionary, iterations=500, workers=cpus)\n",
    "      \n",
    "    cv_coherence_model_mallet_lda = gensim. models.CoherenceModel (model=mallet_lda_model, corpus=corpus, texts=texts,\n",
    "                                                                     dictionary=dictionary, coherence='c_v')\n",
    "      \n",
    "    coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    models.append(mallet_lda_model)\n",
    "  return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus=corpus, texts=data_ready, dictionary=id2word,\n",
    "                                                               start_topic_count=2, end_topic_count=50, step=2, cpus=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 51, 2), 'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "x_ax = range(2, 51, 2)\n",
    "y_ax = coherence_scores\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "\n",
    "plt.axhline(y=0.42, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the Papers published by Bengio Yoshua \n",
    "\n",
    "Bengio Yoshua, is well known for his work on Artifical Neural Networks and Deep Learning. Bengio along with Geoffrey Hinton and Yann LeCun are reffered to as the \"Godfathers of AI\". Let us look at what kind of research Bengio has been involved him and understand his contributions to this field - that led him to win the Turing Award\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_bengio_author']=data['authors'].apply(lambda x:1 if \"Bengio Yoshua\" in x else 0)\n",
    "bengio_papers=data[data['is_bengio_author']==1]\n",
    "bengio_papers=bengio_papers.reset_index(drop=True)\n",
    "\n",
    "print(\"Number of Papers by Bengio Yoshua on Arxiv is \",bengio_papers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bengio Yoshua Published His First Paper in \",min(bengio_papers['Date']))\n",
    "print(\"Bengio Yoshua Published His Recent Paper in \",max(bengio_papers['Date']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though Bengio, had entered the field of AI ML in the 1990's the first paper published by him on Arxiv is in September of 2010 and his most recent paper is in August 2020. In 10 years, he has published 311 papers - Astounding Rate of Publication. It may be possible that his other papers are tagged into other categories on Arxiv that we are not considering for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengio_papers_by_year=bengio_papers.groupby(['Year']).size().reset_index().rename(columns={0:'Number of Papers Published'})\n",
    "\n",
    "px.bar(x=\"Year\",y=\"Number of Papers Published\",title=\"Papers by Bengio Yoshua Over Years\",data_frame=bengio_papers_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Papers Published in a Year By Bengio Yoshua \",np.median(bengio_papers_by_year['Number of Papers Published']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  What are the topics in which Bengio Yoshua has published papers in?\n",
    "\n",
    "To look at topics at a broad level, we can Build a Frequency Bar Plot to understand key words used in the titles of the papers published.\n",
    "\n",
    "Before we look at the top words in the Title, we will have to do some cleaning of the title - Removing Stop Words, Lower Casing the Words. Let us not do any stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=data['title'].tolist()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "titles=[title.lower() for title in titles] ### Lower Casing the Title\n",
    "titles=[utils.removeStopWords(title,stop_words) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigrams_list=[\" \".join(utils.generateNGram(title,2)) for title in titles]\n",
    "topn=50\n",
    "top_bigrams=utils.getMostCommon(bigrams_list,topn=topn)\n",
    "top_bigrams_df=pd.DataFrame()\n",
    "top_bigrams_df['words']=[val[0] for val in top_bigrams]\n",
    "top_bigrams_df['Frequency']=[val[1] for val in top_bigrams]\n",
    "px.bar(data_frame=top_bigrams_df.sort_values(\"Frequency\",ascending=True),x=\"Frequency\",y=\"words\",orientation=\"h\",title=\"Top \"+str(topn)+\" Bigrams in Papers by Bengio Yoshua\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_list=[\" \".join(utils.generateNGram(title.replace(\":\",\"\"),3)) for title in titles]\n",
    "topn=50\n",
    "top_trigrams=utils.getMostCommon(trigrams_list,topn=topn)\n",
    "top_trigrams_df=pd.DataFrame()\n",
    "top_trigrams_df['words']=[val[0] for val in top_trigrams]\n",
    "top_trigrams_df['Frequency']=[val[1] for val in top_trigrams]\n",
    "top_trigrams_df=top_trigrams_df[top_trigrams_df[\"words\"]!=\"\"]\n",
    "px.bar(data_frame=top_trigrams_df.sort_values(\"Frequency\",ascending=True),x=\"Frequency\",y=\"words\",orientation=\"h\",title=\"Top \"+str(topn)+\" Trigrams in Papers by Bengio Yoshua\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can there has been a lot of Papers on Recurrent Neural Networks and Reinforcement Learning by Bengio Yoshua. Also, his research areas are also focussed on Neural Machine Translations and Understanding Stochastic Gradients. The top words also, show us that Bengio has worked on various topics in Deep Learning as part of his research - The next question arises is can we categorise his work? And also can we see who are the authors he works predominantly with for each of the categories we have identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling to Understand Different Themes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The tokenise function will lowercase, and tokenise the sentences\n",
    "'''\n",
    "\n",
    "def tokenise(sentences):\n",
    "    return [gensim.utils.simple_preprocess(sentence, deacc=True,max_len=50) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_sentences=tokenise(bengio_papers['title'].tolist())\n",
    "tokenised_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(sentence,stop_words,allowed_postags=None):\n",
    "    doc=nlp(sentence)\n",
    "    #print(sentence)\n",
    "    if allowed_postags!=None:\n",
    "        tokens = [token.lemma_ for token in doc if (token.pos_ in allowed_postags) and (token.text not in stop_words)]\n",
    "    if allowed_postags==None:\n",
    "        tokens= [token.lemma_ for token in doc if (token.text not in stop_words)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[\" \".join(tokenised_sentence) for tokenised_sentence in tokenised_sentences]\n",
    "lemmatised_sentences=[lemmatise(sentence,stop_words) for sentence in sentences]\n",
    "lemmatised_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Bigrams and Trigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(lemmatised_sentences,min_count=2) \n",
    "trigram = gensim.models.Phrases(bigram[lemmatised_sentences],min_count=2)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_words=[bigram_mod[sentence] for sentence in lemmatised_sentences]\n",
    "\n",
    "trigrams_words=[trigram_mod[sentence] for sentence in bigrams_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dictionary and Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(trigrams_words)\n",
    "corpus = [id2word.doc2bow(text) for text in trigrams_words]\n",
    "[(id2word[id], freq) for id, freq in corpus[0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=20,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models,coherence=compute_coherence_values(id2word,corpus,trigrams_words,limit=20,start=2,step=2)\n",
    "x = range(2, 20, 2)\n",
    "plt.plot(x, coherence)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 6 topics seem a good number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=6, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=20,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.enable_notebook()\n",
    "#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "#vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=trigrams_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are topics related to GAN and adversial networks in relation to speech and images.There are topics related to hypergraph and Deep Reinforcement Learning as well.\n",
    "\n",
    "Let us now assign, each document to a Topics - a document may consists of more than one topic, but we will assign it the dominant topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(texts,ldamodel=lda_model, corpus=corpus):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(bengio_papers['title'].tolist(),ldamodel=lda_model, corpus=corpus)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts=df_dominant_topic['Dominant_Topic'].value_counts().reset_index().rename(columns={'index':'Topic','Dominant_Topic':'Number of Documents'})\n",
    "topic_counts['percentage_contribution']=(topic_counts['Number of Documents']/topic_counts['Number of Documents'].sum())*100\n",
    "topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that number of documents is each topic is almost equally distributed.. Let us use T-SNE to visualise the topics vs document distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df=pd.DataFrame()\n",
    "sent_topics_df['Text']=bengio_papers['title'].tolist()\n",
    "sent_topics_df['tsne_x']=tsne_lda[:,0]\n",
    "sent_topics_df['tsne_y']=tsne_lda[:,1]\n",
    "sent_topics_df['Topic_No']=topic_num\n",
    "sent_topics_df=pd.merge(sent_topics_df,df_dominant_topic,on=\"Text\")\n",
    "sent_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x='tsne_x',y='tsne_y',data_frame=sent_topics_df,color=\"Topic_No\",hover_data=[\"Topic_Perc_Contrib\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics are very well seperated as we can see from TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Has Bengio Worked with Different Authors on Different Topics? Who is the most popular Co-Author across different topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengio_papers=pd.merge(bengio_papers,df_dominant_topic.rename(columns={'Text':'title'}),on='title')\n",
    "\n",
    "num_topics=bengio_papers['Dominant_Topic'].nunique()\n",
    "authors_df_list=[]\n",
    "\n",
    "for topic_no in range(num_topics):\n",
    "    \n",
    "\n",
    "    temp=bengio_papers[bengio_papers['Dominant_Topic']==topic_no]\n",
    "    authors=pd.DataFrame(utils.flattenList(temp['authors'].tolist())).rename(columns={0:'authors'})\n",
    "    authors=authors[authors['authors']!=\"Bengio Yoshua\"]\n",
    "    papers_authors=authors.groupby(['authors']).size().reset_index().rename(columns={0:'Number of Papers Published'}).sort_values(\"Number of Papers Published\",ascending=False).head(10)\n",
    "    papers_authors['Topic No']=topic_no\n",
    "    authors_df_list.append(papers_authors)\n",
    "\n",
    "co_occurring_authors=pd.concat(authors_df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2)\n",
    "row=1\n",
    "col=1\n",
    "for topic_no in range(num_topics):\n",
    "    \n",
    "    wp = lda_model.show_topic(topic_no)\n",
    "    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "    temp=co_occurring_authors.loc[co_occurring_authors['Topic No']==topic_no].sort_values(\"Number of Papers Published\",ascending=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=temp['Number of Papers Published'],\n",
    "        y=temp['authors'],\n",
    "        orientation='h',\n",
    "        name=\"Topic \"+str(topic_no)\n",
    "        #mode=\"markers+text\",\n",
    "        #text=[\"Text A\", \"Text B\", \"Text C\"],\n",
    "        #textposition=\"bottom center\"\n",
    "    ),\n",
    "    row=row, col=col)\n",
    "    if col%2==0:\n",
    "        row=row+1\n",
    "        col=1\n",
    "    else:\n",
    "        col=col+1\n",
    "fig.update_layout(height=1000, width=1200, title_text=\"Top 10 Authors With Whom Bengio Worked Across Different Topics\")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across Topics, Bengio has published papers with Courville Aaron. The other authors are quite distinct across Topics.Courville Aaron is a part of LISA lab along with Bengiom. Except for Topic 2 which talks about self taught deep neural networks and causal networks, the top author with whom Bengio has published his papers with is Courville Aaraon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Works\n",
    "\n",
    "In this Analysis, we started off with analysing the set of AI and ML Papers in the Arxiv Repository. And then we explored the Worked of Bengio Yoshua. As a part of Future Work we can \n",
    "\n",
    "1. Use Abstract Information for more indepth topic Analysis\n",
    "2. Can we build a co-citation network and analyse similar authors\n",
    "3. We can build a Topic Model on Entire Dataset to understand how each topic has evolved over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
